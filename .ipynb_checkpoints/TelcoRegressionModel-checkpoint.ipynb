{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Create new features, handle missing values, perform one-hot encoding, and scale/normalize features using techniques like StandardScaler or MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>tenure</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>customerID_0003-MKNFE</th>\n",
       "      <th>customerID_0004-TLHLJ</th>\n",
       "      <th>customerID_0011-IGKFF</th>\n",
       "      <th>customerID_0013-EXCHZ</th>\n",
       "      <th>customerID_0013-MHZWF</th>\n",
       "      <th>customerID_0013-SMEOE</th>\n",
       "      <th>customerID_0014-BMAQU</th>\n",
       "      <th>...</th>\n",
       "      <th>TotalCharges_996.45</th>\n",
       "      <th>TotalCharges_996.85</th>\n",
       "      <th>TotalCharges_996.95</th>\n",
       "      <th>TotalCharges_997.65</th>\n",
       "      <th>TotalCharges_997.75</th>\n",
       "      <th>TotalCharges_998.1</th>\n",
       "      <th>TotalCharges_999.45</th>\n",
       "      <th>TotalCharges_999.8</th>\n",
       "      <th>TotalCharges_999.9</th>\n",
       "      <th>Churn_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.439916</td>\n",
       "      <td>-1.277445</td>\n",
       "      <td>-1.160323</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.601023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.439916</td>\n",
       "      <td>0.066327</td>\n",
       "      <td>-0.259629</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.601023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.439916</td>\n",
       "      <td>-1.236724</td>\n",
       "      <td>-0.362660</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>1.663829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.439916</td>\n",
       "      <td>0.514251</td>\n",
       "      <td>-0.746535</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.601023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.439916</td>\n",
       "      <td>-1.236724</td>\n",
       "      <td>0.197365</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>-0.011917</td>\n",
       "      <td>1.663829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 13602 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeniorCitizen    tenure  MonthlyCharges  customerID_0003-MKNFE  \\\n",
       "0      -0.439916 -1.277445       -1.160323              -0.011917   \n",
       "1      -0.439916  0.066327       -0.259629              -0.011917   \n",
       "2      -0.439916 -1.236724       -0.362660              -0.011917   \n",
       "3      -0.439916  0.514251       -0.746535              -0.011917   \n",
       "4      -0.439916 -1.236724        0.197365              -0.011917   \n",
       "\n",
       "   customerID_0004-TLHLJ  customerID_0011-IGKFF  customerID_0013-EXCHZ  \\\n",
       "0              -0.011917              -0.011917              -0.011917   \n",
       "1              -0.011917              -0.011917              -0.011917   \n",
       "2              -0.011917              -0.011917              -0.011917   \n",
       "3              -0.011917              -0.011917              -0.011917   \n",
       "4              -0.011917              -0.011917              -0.011917   \n",
       "\n",
       "   customerID_0013-MHZWF  customerID_0013-SMEOE  customerID_0014-BMAQU  ...  \\\n",
       "0              -0.011917              -0.011917              -0.011917  ...   \n",
       "1              -0.011917              -0.011917              -0.011917  ...   \n",
       "2              -0.011917              -0.011917              -0.011917  ...   \n",
       "3              -0.011917              -0.011917              -0.011917  ...   \n",
       "4              -0.011917              -0.011917              -0.011917  ...   \n",
       "\n",
       "   TotalCharges_996.45  TotalCharges_996.85  TotalCharges_996.95  \\\n",
       "0            -0.011917            -0.011917            -0.011917   \n",
       "1            -0.011917            -0.011917            -0.011917   \n",
       "2            -0.011917            -0.011917            -0.011917   \n",
       "3            -0.011917            -0.011917            -0.011917   \n",
       "4            -0.011917            -0.011917            -0.011917   \n",
       "\n",
       "   TotalCharges_997.65  TotalCharges_997.75  TotalCharges_998.1  \\\n",
       "0            -0.011917            -0.011917           -0.011917   \n",
       "1            -0.011917            -0.011917           -0.011917   \n",
       "2            -0.011917            -0.011917           -0.011917   \n",
       "3            -0.011917            -0.011917           -0.011917   \n",
       "4            -0.011917            -0.011917           -0.011917   \n",
       "\n",
       "   TotalCharges_999.45  TotalCharges_999.8  TotalCharges_999.9  Churn_Yes  \n",
       "0            -0.011917           -0.011917           -0.011917  -0.601023  \n",
       "1            -0.011917           -0.011917           -0.011917  -0.601023  \n",
       "2            -0.011917           -0.011917           -0.011917   1.663829  \n",
       "3            -0.011917           -0.011917           -0.011917  -0.601023  \n",
       "4            -0.011917           -0.011917           -0.011917   1.663829  \n",
       "\n",
       "[5 rows x 13602 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "path = 'C:/Users/MSI/.cache/kagglehub/datasets/blastchar/telco-customer-churn/versions/1'\n",
    "# Load dataset\n",
    "data = pd.read_csv(f\"{path}/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "\n",
    "# Handle missing values\n",
    "numeric_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute numeric features with mean strategy\n",
    "imputer_numeric = SimpleImputer(strategy='mean')\n",
    "data[numeric_features] = imputer_numeric.fit_transform(data[numeric_features])\n",
    "\n",
    "# Impute categorical features with most_frequent strategy\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "data[categorical_features] = imputer_categorical.fit_transform(data[categorical_features])\n",
    "\n",
    "data_imputed = data.copy()\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical_features = data_imputed.select_dtypes(include=['object']).columns\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_features = pd.DataFrame(encoder.fit_transform(data_imputed[categorical_features]), columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# Drop original categorical features and concatenate encoded features\n",
    "data_imputed = data_imputed.drop(categorical_features, axis=1)\n",
    "data_encoded = pd.concat([data_imputed, encoded_features], axis=1)\n",
    "\n",
    "# Scale/normalize features\n",
    "scaler = StandardScaler()\n",
    "data_scaled = pd.DataFrame(scaler.fit_transform(data_encoded), columns=data_encoded.columns)\n",
    "\n",
    "# Display the first few rows of the processed data\n",
    "data_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Correlation Analysis\n",
    "Generate correlation matrix, create heatmap visualization, identify and handle multicollinearity between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Correlation Analysis\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate correlation matrix\n",
    "correlation_matrix = data_scaled.corr()\n",
    "\n",
    "# Create heatmap visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Identify and handle multicollinearity\n",
    "# We will drop one of the highly correlated features (correlation > 0.8)\n",
    "threshold = 0.8\n",
    "high_corr_var = np.where(correlation_matrix > threshold)\n",
    "high_corr_var = [(correlation_matrix.columns[x], correlation_matrix.columns[y]) for x, y in zip(*high_corr_var) if x != y and x < y]\n",
    "\n",
    "# Drop one of each pair of highly correlated features\n",
    "features_to_drop = set()\n",
    "for var1, var2 in high_corr_var:\n",
    "    features_to_drop.add(var2)\n",
    "\n",
    "data_reduced = data_scaled.drop(columns=features_to_drop)\n",
    "\n",
    "# Display the first few rows of the reduced data\n",
    "data_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Split data into training and testing sets, initialize the model, train it on the training data, and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split data into features and target variable\n",
    "X = data_reduced.drop('target', axis=1)\n",
    "y = data_reduced['target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Calculate accuracy, precision, recall, F1-score, and generate confusion matrix. Plot ROC curve and calculate AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-score: {f1:.2f}')\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve and calculate AUC score\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "print(f'AUC score: {roc_auc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Analysis\n",
    "Analyze learning curves to detect overfitting/underfitting, perform cross-validation, and validate model performance on different data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Analysis\n",
    "\n",
    "from sklearn.model_selection import learning_curve, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to plot learning curves\n",
    "def plot_learning_curves(estimator, X, y):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves for the model\n",
    "plot_learning_curves(model, X_train, y_train)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "print(f'Cross-validation scores: {cv_scores}')\n",
    "print(f'Mean cross-validation score: {cv_scores.mean():.2f}')\n",
    "\n",
    "# Evaluate model performance on different data splits\n",
    "train_sizes = [0.6, 0.7, 0.8]\n",
    "for train_size in train_sizes:\n",
    "    X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X, y, train_size=train_size, random_state=42)\n",
    "    model.fit(X_train_split, y_train_split)\n",
    "    y_pred_split = model.predict(X_test_split)\n",
    "    accuracy_split = accuracy_score(y_test_split, y_pred_split)\n",
    "    print(f'Train size: {train_size}, Accuracy: {accuracy_split:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
